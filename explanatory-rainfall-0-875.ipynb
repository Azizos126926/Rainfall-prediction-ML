{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91714,"databundleVersionId":11251744,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Rainfall COMPETITION:\n\n## 1. Understand Data\n## 2. Handle missing data (Skip since no data missing)\n## 3. Feature Engineering\n## 4. Compare ML models\n## 5. Parameter Tunning per model\n## 6. Best results","metadata":{"execution":{"iopub.status.busy":"2025-03-18T01:58:57.411559Z","iopub.execute_input":"2025-03-18T01:58:57.411890Z","iopub.status.idle":"2025-03-18T01:58:57.416032Z","shell.execute_reply.started":"2025-03-18T01:58:57.411864Z","shell.execute_reply":"2025-03-18T01:58:57.414821Z"}}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:45.622775Z","iopub.execute_input":"2025-03-19T00:44:45.623331Z","iopub.status.idle":"2025-03-19T00:44:45.634236Z","shell.execute_reply.started":"2025-03-19T00:44:45.623292Z","shell.execute_reply":"2025-03-19T00:44:45.632373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.  Understand Data","metadata":{}},{"cell_type":"code","source":"# Load the data\ntrain_data = pd.read_csv(\"/kaggle/input/playground-series-s5e3/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/playground-series-s5e3/test.csv\")\n\n# Check the data\ntrain_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:45.635608Z","iopub.execute_input":"2025-03-19T00:44:45.635932Z","iopub.status.idle":"2025-03-19T00:44:45.689331Z","shell.execute_reply.started":"2025-03-19T00:44:45.635903Z","shell.execute_reply":"2025-03-19T00:44:45.688346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#quick look at our data types & null counts \ntrain_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:45.691411Z","iopub.execute_input":"2025-03-19T00:44:45.691778Z","iopub.status.idle":"2025-03-19T00:44:45.704568Z","shell.execute_reply.started":"2025-03-19T00:44:45.691749Z","shell.execute_reply":"2025-03-19T00:44:45.703246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:45.706076Z","iopub.execute_input":"2025-03-19T00:44:45.706476Z","iopub.status.idle":"2025-03-19T00:44:45.727266Z","shell.execute_reply.started":"2025-03-19T00:44:45.706439Z","shell.execute_reply":"2025-03-19T00:44:45.726073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure train_data is loaded\n# train_data = pd.read_csv(\"your_file.csv\")  # Uncomment if applicable\n\nX = train_data.copy()\ndays=X.pop(\"day\")\ny = X.pop(\"rainfall\")\nprint(y.value_counts())\nprint(y.value_counts(normalize=True))  # To see proportions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:45.728443Z","iopub.execute_input":"2025-03-19T00:44:45.728808Z","iopub.status.idle":"2025-03-19T00:44:45.751069Z","shell.execute_reply.started":"2025-03-19T00:44:45.728770Z","shell.execute_reply":"2025-03-19T00:44:45.750112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function for MI scores\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.feature_selection import mutual_info_regression\ndef make_mi_scores(X, y, discrete_features):\n    # Ensure X and y contain no NaN values\n     # Ensure X and y contain no NaN values\n    if X.isna().sum().sum() > 0 or y.isna().sum() > 0:\n        raise ValueError(\"X or y contains NaN values!\")\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:45.752461Z","iopub.execute_input":"2025-03-19T00:44:45.752839Z","iopub.status.idle":"2025-03-19T00:44:45.769877Z","shell.execute_reply.started":"2025-03-19T00:44:45.752802Z","shell.execute_reply":"2025-03-19T00:44:45.768749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Label encoding for categorical variables\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# Identify discrete features correctly\ndiscrete_features = X.dtypes == int\nprint(discrete_features)\n\n\nprint(f\"X shape: {X.shape}, y shape: {y.shape}\")\nprint(f\"Missing values in X: {X.isna().sum().sum()}, Missing values in y: {y.isna().sum()}\")\n\nif X.shape[0] == 0 or y.shape[0] == 0:\n    raise ValueError(\"X or y is empty after preprocessing!\")\nmi_scores = mutual_info_regression(X, y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\nmi_scores = mi_scores.sort_values(ascending=False)\n#mi_scores = make_mi_scores(X, y, discrete_features)\nprint(mi_scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:45.771600Z","iopub.execute_input":"2025-03-19T00:44:45.771981Z","iopub.status.idle":"2025-03-19T00:44:45.950492Z","shell.execute_reply.started":"2025-03-19T00:44:45.771951Z","shell.execute_reply":"2025-03-19T00:44:45.949471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysis MI Scores\n\n* We can see that humidity, sunshine and cloud are the most important features to consider. Ofc, other features should be considere like Temperature or pressure. ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Correlation matrix for all features\ncorr_matrix = X.join(y).corr()  # Joining y to X for correlation with target\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title(\"Correlation Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:45.952049Z","iopub.execute_input":"2025-03-19T00:44:45.952455Z","iopub.status.idle":"2025-03-19T00:44:47.046340Z","shell.execute_reply.started":"2025-03-19T00:44:45.952416Z","shell.execute_reply":"2025-03-19T00:44:47.045025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysis correlation matrix\n* We have 3 temperatures variables which are close to each others, maybe fitting the model to these 3 features would help much.\n* The pressure is closely related to the temperature, this probably a thermodynamic property so T and P has some linear mathematical relation (PV = nkBT for example).\n* Wind Speed and Winddirection are closely related to temperature and pressure\n* Sunshine and Cloud are inversly related (that's a bit obvious)","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Fit a random forest model\nrf = RandomForestRegressor(n_estimators=100)\nrf.fit(X, y)\n\n# Plot feature importances\nimportances = rf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(12, 6))\nplt.title(\"Feature Importance\")\nplt.barh(range(X.shape[1]), importances[indices], align=\"center\")\nplt.yticks(range(X.shape[1]), X.columns[indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:47.047446Z","iopub.execute_input":"2025-03-19T00:44:47.047796Z","iopub.status.idle":"2025-03-19T00:44:48.541983Z","shell.execute_reply.started":"2025-03-19T00:44:47.047762Z","shell.execute_reply":"2025-03-19T00:44:48.540936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* This shows what we discussed before but **cloud** is logically the most important feature\n* So main point: we can drop  \"mintemp\", \"maxtemp\", \"winddirection\" since they don't give us more info than the remaining features ","metadata":{}},{"cell_type":"markdown","source":"## 3.  Feature engineering","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom warnings import simplefilter\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\nsimplefilter(\"ignore\")\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 5))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n# annotations: https://stackoverflow.com/a/49238256/5769929\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"365D\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\ntrain_data[\"day\"] = pd.date_range(start=\"2020-01-01\", periods=len(train_data), freq=\"D\")\ntrain_data_tm = train_data.set_index(\"day\").to_period(\"D\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:48.543036Z","iopub.execute_input":"2025-03-19T00:44:48.543348Z","iopub.status.idle":"2025-03-19T00:44:48.567326Z","shell.execute_reply.started":"2025-03-19T00:44:48.543322Z","shell.execute_reply":"2025-03-19T00:44:48.566245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_s = train_data_tm.copy()\n\n# Convert index to datetime if needed\nif isinstance(X_s.index, pd.PeriodIndex):\n    X_s.index = X_s.index.to_timestamp()\n\n# days within a week\nX_s[\"day_s\"] = X_s.index.dayofweek  \nX_s[\"week\"] = X_s.index.isocalendar().week  \n\n# days within a year\n\nX_s[\"dayofyear\"] = X_s.index.dayofyear\nX_s[\"year\"] = X_s.index.year\n\nfig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))\nseasonal_plot(X_s, y=\"rainfall\", period=\"week\", freq=\"day_s\", ax=ax0)  \nX_s[\"month\"] = X_s.index.month\nseasonal_plot(X_s, y=\"rainfall\", period=\"year\", freq=\"month\", ax=ax1)\n# Make sure that X_s has the same length as y\n\nprint(f\"Shape of X_s: {X_s.shape}\")\nX_s.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:48.570249Z","iopub.execute_input":"2025-03-19T00:44:48.570662Z","iopub.status.idle":"2025-03-19T00:44:58.676080Z","shell.execute_reply.started":"2025-03-19T00:44:48.570609Z","shell.execute_reply":"2025-03-19T00:44:58.674943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_periodogram(train_data_tm.rainfall);\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:58.678182Z","iopub.execute_input":"2025-03-19T00:44:58.678557Z","iopub.status.idle":"2025-03-19T00:44:59.406124Z","shell.execute_reply.started":"2025-03-19T00:44:58.678523Z","shell.execute_reply":"2025-03-19T00:44:59.405226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* We can see that the frequency of importance are Annual, semiannual, Quaterly, Bimontly, since we dropped from monthly, this is **in agreement with the cycles of the year (winter, summmer...)** so we can take as frequency bimontly for the seasonality\n* Obvious confirmation but this step could be done in different time depended problem, so important to know","metadata":{}},{"cell_type":"code","source":"print(train_data[\"rainfall\"].value_counts())\nprint(train_data[\"rainfall\"].value_counts(normalize=True))  # To see proportions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:59.407077Z","iopub.execute_input":"2025-03-19T00:44:59.407383Z","iopub.status.idle":"2025-03-19T00:44:59.415435Z","shell.execute_reply.started":"2025-03-19T00:44:59.407358Z","shell.execute_reply":"2025-03-19T00:44:59.414271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Fourier Terms for Bimonthly Periodicity\nX_s['fourier_sin_bimonthly'] = np.sin(2 * np.pi * X_s['dayofyear'] / 60)\nX_s['fourier_cos_bimonthly'] = np.cos(2 * np.pi * X_s['dayofyear'] / 60)\nimport numpy as np\nimport pandas as pd\n\n# Assuming X_s already has the Fourier terms you calculated\n# Let's say you want to create a lag of 1 and 7 days for these features\n\n# Create Lag Features (1-day and 7-day lag)\nlags = [1, 7]\n\n# Loop through each Fourier feature to create lagged versions\nfor lag in lags:\n    # For bimonthly seasonality\n    X_s[f'fourier_sin_bimonthly_lag{lag}'] = X_s['fourier_sin_bimonthly'].shift(lag)\n    X_s[f'fourier_cos_bimonthly_lag{lag}'] = X_s['fourier_cos_bimonthly'].shift(lag)\n\n# Drop rows with NaN values generated due to lagging\nX_s.fillna(method='bfill', inplace=True)  # Use backward fill\n\n# Check the first few rows to see the result\nX_s.head()\n\n\n# Remember to delete unuseful features:\nX_s = X_s.drop(columns=[\"mintemp\", \"sunshine\",\"maxtemp\", \"winddirection\"])\n\n# Check the resulting dataframe\nprint(X_s.head())\n\nprint(f\"X_s shape: {X_s.shape}\")\nX_s.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:59.416498Z","iopub.execute_input":"2025-03-19T00:44:59.416917Z","iopub.status.idle":"2025-03-19T00:44:59.453843Z","shell.execute_reply.started":"2025-03-19T00:44:59.416848Z","shell.execute_reply":"2025-03-19T00:44:59.452878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop 'rainfall' column from X_s before validation\nX_s.drop(columns=[\"rainfall\"], inplace=True)  # Removes only the 'rainfall' column\n\nprint(f\"X_s shape: {X_s.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"X colimns:{X_s.columns}\")\nX_s.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:59.454861Z","iopub.execute_input":"2025-03-19T00:44:59.455263Z","iopub.status.idle":"2025-03-19T00:44:59.479331Z","shell.execute_reply.started":"2025-03-19T00:44:59.455229Z","shell.execute_reply":"2025-03-19T00:44:59.478289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Fourrier series data filled we can think of lag features like for weakly or monthly which might be a good  idea for time series forecasting assuming that test_data is a continuity of the training data timewise, This part good be tried later ","metadata":{}},{"cell_type":"markdown","source":"# 4. Models and Comparaison","metadata":{}},{"cell_type":"markdown","source":"* First attempts ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score, brier_score_loss\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n# Split into train and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X_s, y, test_size=0.2, random_state=42)\n\n# Initialize models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100),\n    \"CatBoost\": CatBoostClassifier(iterations=300, depth=10, learning_rate=0.05, loss_function='Logloss', cat_features=[], verbose=0),\n    \"LightGBM\": LGBMClassifier(n_estimators=300, learning_rate=0.05, max_depth=10, objective='binary', verbose=-1),\n    \"XGBoost\": XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=10, objective='binary:logistic', eval_metric=\"logloss\",verbosity=0),\n    \"HistGradientBoosting\": HistGradientBoostingClassifier(max_iter=300, learning_rate=0.05, max_depth=10)\n}\n\n# Dictionary to store scores\nmodel_scores = {}\n\n# Evaluate models\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    \n    # Predict probabilities\n    prob_train = model.predict_proba(X_train)[:, 1]\n    prob_valid = model.predict_proba(X_valid)[:, 1]\n    \n    # Evaluate performance\n    auc_train = roc_auc_score(y_train, prob_train)\n    auc_valid = roc_auc_score(y_valid, prob_valid)\n    \n    # Store the results\n    model_scores[name] = {\n        \"Train AUC\": auc_train,\n        \"Valid AUC\": auc_valid,\n    }\n\n# Display model comparison\nmodel_scores_df = pd.DataFrame(model_scores).T\nprint(model_scores_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:44:59.480376Z","iopub.execute_input":"2025-03-19T00:44:59.480730Z","iopub.status.idle":"2025-03-19T00:45:06.373008Z","shell.execute_reply.started":"2025-03-19T00:44:59.480693Z","shell.execute_reply":"2025-03-19T00:45:06.372065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ANALYSIS\n* Waw logistic regression might be the one but mainly we can see that **Logistic Regression, Random Forest, Gradient Boosting**  are the main winners maybe with parameter tunning we can have better results or maybe ensemble classifiers\n* Several models (Random Forest, CatBoost, LightGBM, XGBoost, HistGradientBoosting) show a perfect train AUC of 1.0. This is often an indicator of **overfitting**, where the model is memorizing the training data but not generalizing well to the validation data.\nThis suggests that these models might benefit from regularization or hyperparameter tuning to reduce overfitting.\nLogistic Regression and Gradient Boosting are less prone to overfitting compared to others, as their train AUCs are not perfectly 1.0.\n* The best validation AUC score comes from Logistic Regression with 0.869. This indicates that despite its simple nature, it is generalizing well to unseen data.\n* Random Forest, Gradient Boosting, CatBoost, and HistGradientBoosting all have validation AUCs around 0.85, suggesting they are performing similarly but are likely overfitting the training data.\n* XGBoost and LightGBM have slightly lower validation AUCs (around 0.83), but their performance is still competitive.","metadata":{}},{"cell_type":"markdown","source":"# 5. Hyperparameter Tuning and Suggestions","metadata":{}},{"cell_type":"markdown","source":"1. Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nlog_reg = LogisticRegression(max_iter=1000)\n\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100],   # Regularization strength\n    'penalty': ['l2'],               # Regularization type\n    'solver': ['liblinear']          # Solver for small datasets\n}\n\ngrid_search = GridSearchCV(log_reg, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\ngrid_search.fit(X_s, y)\n\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best AUC: \", grid_search.best_score_)\n# Logistic Regression Results (After Hyperparameter Tuning)\nlog_reg_best = grid_search.best_score_\nlog_reg_grid = grid_search.best_params_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:45:06.373840Z","iopub.execute_input":"2025-03-19T00:45:06.374109Z","iopub.status.idle":"2025-03-19T00:45:08.411412Z","shell.execute_reply.started":"2025-03-19T00:45:06.374086Z","shell.execute_reply":"2025-03-19T00:45:08.410081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Random forest ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nrf = RandomForestClassifier(n_estimators=100)\n\nparam_grid = {\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt'],\n}\n\ngrid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\ngrid_search.fit(X_s, y)\n\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best AUC: \", grid_search.best_score_)\n# Random Forest Results (After Hyperparameter Tuning)\nrf_best = grid_search.best_score_\nrf_grid = grid_search.best_params_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:45:08.412681Z","iopub.execute_input":"2025-03-19T00:45:08.413050Z","iopub.status.idle":"2025-03-19T00:45:55.472844Z","shell.execute_reply.started":"2025-03-19T00:45:08.413019Z","shell.execute_reply":"2025-03-19T00:45:55.471692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3.  Gradient Boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngb = GradientBoostingClassifier(n_estimators=100)\n\nparam_grid = {\n    'learning_rate': [0.05, 0.1, 0.2],\n    'max_depth': [3, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n}\n\ngrid_search = GridSearchCV(gb, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\ngrid_search.fit(X_s, y)\n\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best AUC: \", grid_search.best_score_)\n# Gradient Boosting Results (After Hyperparameter Tuning)\ngb_best = grid_search.best_score_\ngb_grid = grid_search.best_params_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:45:55.473793Z","iopub.execute_input":"2025-03-19T00:45:55.474140Z","iopub.status.idle":"2025-03-19T00:48:52.777957Z","shell.execute_reply.started":"2025-03-19T00:45:55.474112Z","shell.execute_reply":"2025-03-19T00:48:52.777019Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. CatBoost","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncatboost = CatBoostClassifier(iterations=300, cat_features=[], verbose=0)\n\nparam_grid = {\n    'depth': [6, 8, 10],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'l2_leaf_reg': [1, 3, 5],\n    'border_count': [32, 64],\n}\n\ngrid_search = GridSearchCV(catboost, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\ngrid_search.fit(X_s, y)\n\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best AUC: \", grid_search.best_score_)\n# CatBoost Results (After Hyperparameter Tuning)\ncatboost_best = grid_search.best_score_\ncatboost_grid = grid_search.best_params_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:48:52.778951Z","iopub.execute_input":"2025-03-19T00:48:52.779316Z","iopub.status.idle":"2025-03-19T00:53:05.068426Z","shell.execute_reply.started":"2025-03-19T00:48:52.779290Z","shell.execute_reply":"2025-03-19T00:53:05.067280Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. LightGBM","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(n_estimators=300,verbose=-1)\n\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [5, 10, 20],\n    'num_leaves': [31, 50, 100],\n    'min_data_in_leaf': [20, 40],\n}\n\ngrid_search = GridSearchCV(lgbm, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\ngrid_search.fit(X_s, y)\n\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best AUC: \", grid_search.best_score_)\n# LightGBM Results (After Hyperparameter Tuning)\nlgbm_best = grid_search.best_score_\nlgbm_grid = grid_search.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:53:05.069695Z","iopub.execute_input":"2025-03-19T00:53:05.070085Z","iopub.status.idle":"2025-03-19T00:55:55.867295Z","shell.execute_reply.started":"2025-03-19T00:53:05.070048Z","shell.execute_reply":"2025-03-19T00:55:55.866286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"6. XGB ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_estimators=300,verbosity=0)\n\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 6, 10],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 1.0],\n}\n\ngrid_search = GridSearchCV(xgb, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\ngrid_search.fit(X_s, y)\n\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best AUC: \", grid_search.best_score_)\n# XGBoost Results (After Hyperparameter Tuning)\nxgb_best = grid_search.best_score_\nxgb_grid = grid_search.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:55:55.870374Z","iopub.execute_input":"2025-03-19T00:55:55.870630Z","iopub.status.idle":"2025-03-19T00:56:51.618389Z","shell.execute_reply.started":"2025-03-19T00:55:55.870609Z","shell.execute_reply":"2025-03-19T00:56:51.617283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"7.  HistGradientBoosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingClassifier\n\nhgb = HistGradientBoostingClassifier(max_iter=300)\n\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 6, 10],\n    'min_samples_leaf': [20, 40],\n}\n\ngrid_search = GridSearchCV(hgb, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\ngrid_search.fit(X_s, y)\n\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best AUC: \", grid_search.best_score_)\n# HistGradientBoosting Results (After Hyperparameter Tuning)\nhgb_best = grid_search.best_score_\nhgb_grid = grid_search.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:56:51.619760Z","iopub.execute_input":"2025-03-19T00:56:51.620193Z","iopub.status.idle":"2025-03-19T00:57:20.985545Z","shell.execute_reply.started":"2025-03-19T00:56:51.620138Z","shell.execute_reply":"2025-03-19T00:57:20.983862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now store the results in a single dictionary, including validation and test AUC\nbest_results = {\n    \"Model\": [\n        \"Logistic Regression\", \"Random Forest\", \"Gradient Boosting\", \n        \"CatBoost\", \"LightGBM\", \"XGBoost\", \"HistGradientBoosting\"\n    ],\n    \"Validation AUC\": [\n        log_reg_best, rf_best, gb_best, \n        catboost_best, lgbm_best, xgb_best, hgb_best\n    ],\n    \"Test AUC\": [\n        0.84955, 0.8751, 0.8192, 0.8584, 0.8456, 0.83896, 0.84633  # Add the test results here\n    ],\n    \"Best Parameters\": [\n        log_reg_grid, rf_grid, gb_grid, \n        catboost_grid, lgbm_grid, xgb_grid, hgb_grid\n    ]\n}\n\n# Convert to DataFrame\nresults_df = pd.DataFrame(best_results)\n\n# Display the table\nfrom tabulate import tabulate\nprint(\"\\nHyperparameter Tuning Results:\\n\")\nprint(tabulate(results_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:57:20.986855Z","iopub.execute_input":"2025-03-19T00:57:20.987220Z","iopub.status.idle":"2025-03-19T00:57:21.023357Z","shell.execute_reply.started":"2025-03-19T00:57:20.987182Z","shell.execute_reply":"2025-03-19T00:57:21.022219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ‚≠ê Best model: \n\n# Validation: Logistic Regression with AUC: 0.893058   \n# TEST: After submission we get RandomForest with 0.875","metadata":{}},{"cell_type":"markdown","source":"# Improvement ideas: \n* lag features improvement and time series \n* Feature selection: I assumed that the 2 values of temperatures aren't important but maybe we can add the difference as a feature along with the temperature. Another case, maybe we can ignore sunshine and keep only cloud or use just a combined feature.\n* Try a time independant analysis (saw this idea in discussions)\n","metadata":{}},{"cell_type":"markdown","source":"# 6. Final Results","metadata":{}},{"cell_type":"markdown","source":"Preprocess X_test to be in the same format as our trainning preprocessed format","metadata":{}},{"cell_type":"code","source":"# Now, generate X_test by continuing the date range from the last date of train_data\nlast_date = X_s.index[-1]  # The last date in train_data\nn_test_samples = len(test_data)  # The number of samples in the test data\n\n# Generate the \"day\" column for X_test, starting from the next day\ntest_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=n_test_samples, freq=\"D\")\n\n# Add the \"day\" column to X_test and continue the feature extraction process\ntest_data[\"day\"] = test_dates\ntunnel_test = test_data.set_index(\"day\").to_period(\"D\")\n\nX_test = tunnel_test.copy()\n# Convert index to datetime if needed\nif isinstance(X_test.index, pd.PeriodIndex):\n    X_test.index = X_test.index.to_timestamp()\n\n# Extract the same time-based features for X_test as you did for X_s\nX_test[\"day_s\"] = X_test.index.dayofweek\nX_test[\"week\"] = X_test.index.isocalendar().week\nX_test[\"dayofyear\"] = X_test.index.dayofyear\nX_test[\"year\"] = X_test.index.year\nX_test[\"month\"] = X_test.index.month\nX_test['fourier_sin_bimonthly'] = np.sin(2 * np.pi * X_test['dayofyear'] / 60)\nX_test['fourier_cos_bimonthly'] = np.cos(2 * np.pi * X_test['dayofyear'] / 60)\n# Create Lag Features (1-day and 7-day lag)\nlags = [1, 7]\n\n# Loop through each Fourier feature to create lagged versions\nfor lag in lags:\n    # For bimonthly seasonality\n    X_test[f'fourier_sin_bimonthly_lag{lag}'] = X_test['fourier_sin_bimonthly'].shift(lag)\n    X_test[f'fourier_cos_bimonthly_lag{lag}'] = X_test['fourier_cos_bimonthly'].shift(lag)\n\n# Drop rows with NaN values generated due to lagging\nX_test.fillna(method='bfill', inplace=True)  # Use backward fill\n# Remember to delete unuseful features:\nX_test = X_test.drop(columns=[\"mintemp\",\"sunshine\", \"maxtemp\", \"winddirection\"])\nX_test.columns\n\n# Now, X_test is ready with the same features as X_s for making predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:58:34.525637Z","iopub.execute_input":"2025-03-19T00:58:34.525989Z","iopub.status.idle":"2025-03-19T00:58:34.550836Z","shell.execute_reply.started":"2025-03-19T00:58:34.525961Z","shell.execute_reply":"2025-03-19T00:58:34.549776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\n# Best hyperparameters for each model, replaced with corresponding variables\nbest_params = {\n    \"Logistic Regression\": log_reg_grid,\n    \"Random Forest\": rf_grid,\n    \"Gradient Boosting\": gb_grid,\n    \"CatBoost\": catboost_grid,\n    \"LightGBM\": lgbm_grid,\n    \"XGBoost\": xgb_grid,\n    \"HistGradientBoosting\": hgb_grid\n}\n\n# Define models\nmodels = {\n    \"Logistic Regression\": LogisticRegression,\n    \"Random Forest\": RandomForestClassifier,\n    \"Gradient Boosting\": GradientBoostingClassifier,\n    \"CatBoost\": CatBoostClassifier,\n    \"LightGBM\": LGBMClassifier,\n    \"XGBoost\": XGBClassifier,\n    \"HistGradientBoosting\": HistGradientBoostingClassifier\n}\n\n# Loop through each model, train it, and save predictions\nfor model_name, model_class in models.items():\n    print(f\"Training {model_name}...\")\n\n    # Initialize model with best parameters\n    model = model_class(**best_params[model_name])\n\n    # Train the model\n    model.fit(X_s, y)\n\n    # Make predictions\n    y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probability scores\n\n    # Save predictions to CSV\n    final_predictions = pd.DataFrame({\n        \"id\": test_data[\"id\"],  # Ensure test_data contains 'id' column\n        \"rainfall_probability\": y_pred_prob\n    })\n    \n    # Save to CSV\n    filename = f\"{model_name.replace(' ', '_').lower()}_best_predictions.csv\"\n    final_predictions.to_csv(filename, index=False)\n    print(f\"Saved predictions to {filename}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:58:40.521712Z","iopub.execute_input":"2025-03-19T00:58:40.522056Z","iopub.status.idle":"2025-03-19T00:58:43.893920Z","shell.execute_reply.started":"2025-03-19T00:58:40.522029Z","shell.execute_reply":"2025-03-19T00:58:43.892876Z"}},"outputs":[],"execution_count":null}]}